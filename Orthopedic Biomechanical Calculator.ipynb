{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Disk Hernia and Spondilolysthesis Classification\n",
    "\n",
    "**Data Set Information:**\n",
    "Biomedical data set built by Dr. Henrique da Mota during a medical residence period in the Group of Applied Research in Orthopaedics (GARO) of the Centre MÃ©dico-Chirurgical de RÃ©adaptation des Massues, Lyon, France. The data have been organized in two different but related classification tasks. The first task consists in classifying patients as belonging to one out of three categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150 patients). For the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients). We provide files also for use within the WEKA environment.\n",
    "\n",
    "**Attribute Information:**\n",
    "Each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (in this order): pelvic incidence, pelvic tilt, lumbar lordosis angle, sacral slope, pelvic radius and grade of spondylolisthesis. The following convention is used for the class labels: DH (Disk Hernia), Spondylolisthesis (SL), Normal (NO) and Abnormal (AB).\n",
    "\n",
    "**Dataset Source:**\n",
    "* Guilherme de Alencar Barreto (guilherme '@' deti.ufc.br) & Ajalmar RÃªgo da Rocha Neto (ajalmar '@' ifce.edu.br), Department of Teleinformatics Engineering, Federal University of CearÃ¡, Fortaleza, Ceará¡, Brazil.\n",
    "* Henrique Antonio Fonseca da Mota Filho (hdamota '@' gmail.com), Hospital Monte Klinikum, Fortaleza, Ceará¡, Brazil.\n",
    "* Kaggle Link - https://www.kaggle.com/caesarlupum/vertebralcolumndataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SKLearn libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Sagemaker libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "#PyTorch libraries\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data\n",
    "\n",
    "Let's read in the CSV file and take a look at some of the entries and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the CSV file\n",
    "data_file = 'data/biomechanical_data.csv'\n",
    "data = pd.read_csv(data_file, header=0, delimiter=\",\") \n",
    "\n",
    "#Check out the first few entries\n",
    "data.head(10)\n",
    "\n",
    "#Check out the distribution\n",
    "sns.countplot(x='class', data=data)\n",
    "\n",
    "#Print out some stats about the data\n",
    "print('Number of Patients: ', data.shape[0])\n",
    "counts_per_class=data.groupby(['class']).size()\n",
    "display(counts_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Here we need to split the dataset into a training and testing set. We will use the `MinMaxScaler()` to noramlize the data and change the values of numeric columns in the dataset to a common scale. Then, we will convert the classes into a numeric index.\n",
    "* Normal - 0 \n",
    "* Hernia - 1\n",
    "* Spondylolisthesis - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train, validation, and test data\n",
    "features = data[data.columns[:-1]]\n",
    "labels = data[data.columns[-1]]\n",
    "\n",
    "#Split into train and test\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            stratify=labels,\n",
    "                                                                            random_state=69)\n",
    "\n",
    "#Check the size of the datasets\n",
    "print('Size of training set: ', len(train_features))\n",
    "print('Size of test set: ', len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the classes to a numerical index \n",
    "def class2index(df):\n",
    "    class2idx = {\n",
    "        'Normal':0,\n",
    "        'Hernia':1,\n",
    "        'Spondylolisthesis':2\n",
    "    }\n",
    "    \n",
    "    idx2class = {v: k for k, v in class2idx.items()}\n",
    "    return df.replace(class2idx)\n",
    "\n",
    "train_features, train_labels = np.array(train_features), np.array(class2index(train_labels))\n",
    "test_features, test_labels = np.array(test_features), np.array(class2index(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of instances of each class \n",
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"Normal\": 0,\n",
    "        \"Hernia\": 0,\n",
    "        \"Spondylolisthesis\": 0,\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['Normal'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['Hernia'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['Spondylolisthesis'] += 1          \n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict\n",
    "\n",
    "#Take a look at the distribution to make sure the training and test sets aren't skewed \n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,7))\n",
    "\n",
    "# Train\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(train_labels)]).melt(),\n",
    "            x = \"variable\", y=\"value\", hue=\"variable\",\n",
    "            ax=axes[0]).set_title('Class Distribution in Train Set')\n",
    "\n",
    "# Test\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(test_labels)]).melt(),\n",
    "            x = \"variable\", y=\"value\", hue=\"variable\",\n",
    "            ax=axes[1]).set_title('Class Distribution in Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSVs and Load Data to S3\n",
    "We will create two files: a `training.csv` and `test.csv` file with the features and class labels for the biomechanical data.\n",
    "\n",
    "Save your train and test .csv feature files, locally. Then you can upload local files to S3 by using sagemaker_session.upload_data and pointing directly to where the training data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    # make data dir, if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    pd.concat([pd.DataFrame(y), pd.DataFrame(x)], axis=1).to_csv(os.path.join(data_dir, filename), header=False, index=False)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CSV files for the training and test datasets\n",
    "data_dir = 'training-data'\n",
    "\n",
    "make_csv(train_features, train_labels, filename='train.csv', data_dir=data_dir)\n",
    "make_csv(test_features, test_labels, filename='test.csv', data_dir=data_dir)\n",
    "\n",
    "#Create Sagemaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#Create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "#Set prefix, a descriptive name for a directory  \n",
    "prefix = 'biomechanical-data'\n",
    "\n",
    "#Upload all data to S3\n",
    "data = sagemaker_session.upload_data(path = data_dir, bucket = bucket, key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the 'train.py' function we specified below. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0007\n",
    "NUM_FEATURES = 6\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='pytorch',\n",
    "    role=role,\n",
    "    framework_version='1.0',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    hyperparameters={\n",
    "        'input_features': NUM_FEATURES,\n",
    "        'output_dim': NUM_CLASSES,\n",
    "        'epochs': EPOCHS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy the Model\n",
    "Train the estimator on the training data stored in S3. This should create a training job that we can monitor in the SageMaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01 02:47:22 Starting - Starting the training job...\n",
      "2021-01-01 02:47:46 Starting - Launching requested ML instancesProfilerReport-1609469241: InProgress\n",
      "......\n",
      "2021-01-01 02:48:46 Starting - Preparing the instances for training......\n",
      "2021-01-01 02:49:49 Downloading - Downloading input data\n",
      "2021-01-01 02:49:49 Training - Downloading the training image...\n",
      "2021-01-01 02:50:09 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:10,644 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:10,647 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:10,659 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:13,741 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:14,007 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:14,007 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:14,008 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:14,008 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_5vi53ul/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:16,031 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:16,043 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_features\": 6,\n",
      "        \"epochs\": 20,\n",
      "        \"output_dim\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-01-01-02-47-21-729\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-255753059254/sagemaker-pytorch-2021-01-01-02-47-21-729/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":20,\"input_features\":6,\"output_dim\":3}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-255753059254/sagemaker-pytorch-2021-01-01-02-47-21-729/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":20,\"input_features\":6,\"output_dim\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2021-01-01-02-47-21-729\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-255753059254/sagemaker-pytorch-2021-01-01-02-47-21-729/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"20\",\"--input_features\",\"6\",\"--output_dim\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_FEATURES=6\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 20 --input_features 6 --output_dim 3\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mtorch.Size([217])\u001b[0m\n",
      "\u001b[34mtensor([0.0143, 0.0238, 0.0095])\u001b[0m\n",
      "\u001b[34m<class 'torch.utils.data.dataloader.DataLoader'>\u001b[0m\n",
      "\u001b[34m<class 'torch.Tensor'>\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 0.7522204866011938\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 0.4957253535588582\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 0.47580534633662963\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.5970722999837663\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.4241895435584916\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.5230955514642928\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.4339910190966394\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.3640887414415677\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.4636903504530589\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.4636003027359645\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 0.5417747100194296\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 0.5353814727730222\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 0.4027876216504309\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 0.433978130420049\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 0.4218819464246432\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 0.511107451385922\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 0.4038502391841676\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 0.49479306985934574\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 0.48022953338093227\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 0.4414345274368922\u001b[0m\n",
      "\u001b[34m2021-01-01 02:50:19,734 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-01-01 02:50:48 Uploading - Uploading generated training model\n",
      "2021-01-01 02:50:48 Completed - Training job completed\n",
      "Training seconds: 57\n",
      "Billable seconds: 57\n",
      "CPU times: user 501 ms, sys: 56.9 ms, total: 557 ms\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train estimator on S3 training data\n",
    "estimator.fit({'train':data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 419 ms, sys: 13.9 ms, total: 433 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    framework_version='1.0',\n",
    "    py_version='py3',\n",
    "    entry_point='predict.py',\n",
    "    source_dir='pytorch'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "Once the model is deployed, we can see how it performs when applied to our test data.\n",
    "\n",
    "The provided cell below, reads in the test data, assuming it is stored locally in data_dir and named test.csv. The labels and features are extracted from the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n",
    "\n",
    "#Labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "#Generate predicted, class labels\n",
    "test_y_preds = predictor.predict(test_x)\n",
    "\n",
    "#Test that your model generates the correct number of labels\n",
    "assert len(test_y_preds)==len(test_y), 'Unexpected number of predictions.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class labels: \n",
      "tensor([0, 0, 2, 0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0, 1, 2, 0, 0, 0, 2,\n",
      "        2, 0, 0, 1, 2, 0, 1, 2, 1, 2, 1, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2, 2, 1, 2,\n",
      "        0, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 1, 0, 2, 2, 2, 2, 1, 2, 1,\n",
      "        1, 2, 2, 2, 0, 1, 0, 0, 2, 0, 2, 0, 1, 2, 0, 1, 0, 2, 1, 2, 2])\n",
      "\n",
      "True class labels: \n",
      "[0 0 2 0 1 2 1 2 2 2 2 2 1 2 2 2 1 0 1 2 0 0 0 2 2 1 0 1 2 0 1 2 1 2 1 2 0\n",
      " 0 0 1 2 2 2 0 2 2 1 0 0 2 2 2 0 2 2 0 2 2 1 1 2 0 0 1 0 2 2 2 2 1 2 1 0 2\n",
      " 2 2 0 2 0 0 2 0 2 0 0 2 0 0 0 2 1 2 2]\n",
      "\n",
      "Accuracy: \n",
      "0.8817204301075269\n"
     ]
    }
   ],
   "source": [
    "_, y_pred_tags = torch.max(torch.from_numpy(test_y_preds), dim = 1)\n",
    "print('\\nPredicted class labels: ')\n",
    "print(y_pred_tags)\n",
    "print('\\nTrue class labels: ')\n",
    "print(test_y.values)\n",
    "\n",
    "#Calculate the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_y, y_pred_tags)\n",
    "\n",
    "print('\\nAccuracy: ')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up Resources\n",
    "After we're done evaluating our model, delete the model endpoint. We can do this with a call to `.delete_endpoint()`. Any other resources, we may delete from the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
